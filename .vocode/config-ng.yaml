# Default Vocode configuration
logging:
  default_level: debug
  enabled_loggers:
    LiteLLM: warning
variables:
  LLM_DISCOVERY_MODEL: gpt-5-mini
  LLM_ARCHITECT_MODEL: gpt-5.1
#  LLM_ARCHITECT_REASONING: high
  LLM_ARCHITECT_REASONING: medium
  LLM_CODER_MODEL: gpt-5-mini
# OpenAI
#  LLM_DISCOVERY_MODEL: openai/gpt-5-mini
#  LLM_ARCHITECT_MODEL: openai/gpt-5
#  LLM_CODER_MODEL: openai/gpt-5-mini
# Gemini
#  LLM_DISCOVERY_MODEL: gemini/gemini-2.5-flash
#  LLM_ARCHITECT_MODEL: gemini/gemini-2.5-pro
#  LLM_CODER_MODEL: gemini/gemini-2.5-flash
#  LLM_DIFF_FORMAT: patch
default_workflow: "chat"
tools:
  - name: read_files
    auto_approve: true
  - name: summarize_files
    auto_approve: true
  - name: list_files
    auto_approve: true
  - name: search_project
    auto_approve: true
  - name: run_agent
    auto_approve: true
workflows:
  # Simple chat
  chat:
    description: "General chat assistant that can also browse and read repository files when needed."
    nodes:
      - name: input
        type: input
        message: "How can I help you?"
        outcomes:
          - name: done
      - name: llm
        type: llm
        model: ${LLM_ARCHITECT_MODEL}
        reset_policy: keep
        tools:
          - summarize_files
          - read_files
          - search_project
        outcomes:
          - name: done
    edges:
      - input.done -> llm
      - llm.done -> input
